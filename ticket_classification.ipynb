{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Support Ticket Classification & Prioritization\n",
                "### Machine Learning Task 2 (2026)\n",
                "\n",
                "In this notebook, we build an ML system to automatically classify and prioritize customer support tickets. This helps businesses:\n",
                "- **Respond Faster**: Categorized tickets reach the right team instantly.\n",
                "- **Reduce Backlog**: Automated sorting eliminates manual effort.\n",
                "- **Improve Satisfaction**: High-priority issues are addressed first."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "import string\n",
                "import re\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "import joblib\n",
                "\n",
                "# Pre-download NLP assets\n",
                "nltk.download('stopwords')\n",
                "nltk.download('punkt')\n",
                "nltk.download('wordnet')\n",
                "nltk.download('punkt_tab')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading & Exploratory Data Analysis (EDA)\n",
                "We start by loading the dataset and examining its structure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv('customer_support_tickets.csv')\n",
                "print(f\"Total Tickets: {df.shape[0]}\")\n",
                "df[['Ticket Subject', 'Ticket Description', 'Ticket Type', 'Ticket Priority']].head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Text Preprocessing\n",
                "Raw text is messy. We need to clean it by:\n",
                "- Lowercasing\n",
                "- Removing punctuation and numbers\n",
                "- Removing 'stopwords' (common words like 'the', 'is' that don't add meaning)\n",
                "- **Lemmatization**: Reducing words to their base form (e.g., 'charging' -> 'charge')."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "stop_words = set(stopwords.words('english'))\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "\n",
                "def clean_text(text):\n",
                "    if pd.isna(text):\n",
                "        return \"\"\n",
                "    text = text.lower()\n",
                "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
                "    text = re.sub(r'\\d+', '', text)\n",
                "    tokens = word_tokenize(text)\n",
                "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
                "    return \" \".join(tokens)\n",
                "\n",
                "# Combine Subject and Description for better context\n",
                "df['combined_text'] = df['Ticket Subject'].fillna('') + \" \" + df['Ticket Description'].fillna('')\n",
                "df['cleaned_text'] = df['combined_text'].apply(clean_text)\n",
                "\n",
                "print(\"Sample Cleaned Text:\")\n",
                "print(df['cleaned_text'].iloc[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Feature Extraction & Model Training\n",
                "We use **TF-IDF** to convert text into numbers and **Random Forest** for classification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vectorization\n",
                "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
                "X = tfidf.fit_transform(df['cleaned_text'])\n",
                "\n",
                "# Pre-train Category Model\n",
                "le_type = LabelEncoder()\n",
                "y_type = le_type.fit_transform(df['Ticket Type'])\n",
                "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X, y_type, test_size=0.2, random_state=42)\n",
                "\n",
                "model_type = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "model_type.fit(X_train_t, y_train_t)\n",
                "\n",
                "# Pre-train Priority Model\n",
                "le_prio = LabelEncoder()\n",
                "y_prio = le_prio.fit_transform(df['Ticket Priority'])\n",
                "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(X, y_prio, test_size=0.2, random_state=42)\n",
                "\n",
                "model_prio = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "model_prio.fit(X_train_p, y_train_p)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Evaluation & Visualizations\n",
                "We visualize the performance using **Confusion Matrices**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_evaluation(model, X_test, y_test, classes, title):\n",
                "    y_pred = model.predict(X_test)\n",
                "    cm = confusion_matrix(y_test, y_pred)\n",
                "    \n",
                "    plt.figure(figsize=(8, 6))\n",
                "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap='Blues')\n",
                "    plt.xlabel('Predicted')\n",
                "    plt.ylabel('Actual')\n",
                "    plt.title(f'Confusion Matrix: {title}')\n",
                "    plt.show()\n",
                "    \n",
                "    print(f\"\\nClassification Report for {title}:\")\n",
                "    print(classification_report(y_test, y_pred, target_names=classes))\n",
                "\n",
                "plot_evaluation(model_type, X_test_t, y_test_t, le_type.classes_, \"Ticket Category\")\n",
                "plot_evaluation(model_prio, X_test_p, y_test_p, le_prio.classes_, \"Ticket Priority\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Inference: Testing the System\n",
                "Let's see how the model handles a new, unseen ticket."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_ticket(subject, description):\n",
                "    text = clean_text(subject + \" \" + description)\n",
                "    vec = tfidf.transform([text])\n",
                "    \n",
                "    cat = le_type.inverse_transform(model_type.predict(vec))[0]\n",
                "    prio = le_prio.inverse_transform(model_prio.predict(vec))[0]\n",
                "    \n",
                "    return f\"Category: {cat} | Priority: {prio}\"\n",
                "\n",
                "new_subject = \"Login error\"\n",
                "new_desc = \"I cannot access my account because the password reset link is not working.\"\n",
                "print(f\"Ticket: {new_subject}\")\n",
                "print(predict_ticket(new_subject, new_desc))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
